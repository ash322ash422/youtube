{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec532b7a-5b2d-4e1b-b05e-b5718e89ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1035959d-baa9-4092-9867-1753654955a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y):\n",
    "    m = 0 # Parameter chosen randomly\n",
    "    b = 0 # Parameter chosen randomly\n",
    "    \n",
    "    learn_rate = 0.01 # change from  0.1, 0.01, 0.05 This is hyperparamter\n",
    "    n = len(x)\n",
    "    for i in range(1000): # change 100, 1000\n",
    "        y_predict =  b + m * x # here we have to find b and m\n",
    "        \n",
    "        cost = 1/n * sum([val**2 for val in (y-y_predict)]) # Mean Squared Error\n",
    "        print (i, round(m,2), round(b,2), round(cost,2))\n",
    "        \n",
    "        m_deriv = -(2/n)*sum(x*(y-y_predict)) # differentiate Cost w.r.t. m\n",
    "        b_deriv = -(2/n)*sum(y-y_predict)     # differentiate Cost w.r.t. b\n",
    "                \n",
    "        m = m - learn_rate * m_deriv # update m\n",
    "        b = b - learn_rate * b_deriv # update b\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2e96fe99-a4c2-4da4-88bf-c48d2d76e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([5, 30, 20, 40, 60])\n",
    "\n",
    "# From traditional method we know that actual value of b=12 and m = -5 gives lowest cost=280/5=56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6550e831-b3ff-475c-830e-e3d207f9ab05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 1305.0\n",
      "1 2.34 0.62 788.32\n",
      "2 4.13 1.09 487.2\n",
      "3 5.49 1.44 311.68\n",
      "4 6.54 1.7 209.38\n",
      "5 7.34 1.89 149.73\n",
      "6 7.95 2.03 114.94\n",
      "7 8.42 2.14 94.63\n",
      "8 8.78 2.21 82.77\n",
      "9 9.06 2.26 75.83\n",
      "10 9.27 2.29 71.76\n",
      "11 9.43 2.31 69.36\n",
      "12 9.56 2.32 67.93\n",
      "13 9.66 2.32 67.07\n",
      "14 9.73 2.31 66.54\n",
      "15 9.79 2.3 66.2\n",
      "16 9.84 2.29 65.98\n",
      "17 9.88 2.27 65.82\n",
      "18 9.91 2.25 65.7\n",
      "19 9.93 2.23 65.61\n",
      "20 9.95 2.21 65.53\n",
      "21 9.97 2.19 65.45\n",
      "22 9.99 2.17 65.38\n",
      "23 10.0 2.15 65.31\n",
      "24 10.01 2.12 65.25\n",
      "25 10.02 2.1 65.19\n",
      "26 10.03 2.08 65.12\n",
      "27 10.04 2.05 65.06\n",
      "28 10.05 2.03 65.0\n",
      "29 10.06 2.01 64.94\n",
      "30 10.06 1.98 64.88\n",
      "31 10.07 1.96 64.82\n",
      "32 10.08 1.94 64.76\n",
      "33 10.08 1.91 64.7\n",
      "34 10.09 1.89 64.64\n",
      "35 10.1 1.87 64.58\n",
      "36 10.1 1.84 64.52\n",
      "37 10.11 1.82 64.47\n",
      "38 10.12 1.8 64.41\n",
      "39 10.12 1.77 64.35\n",
      "40 10.13 1.75 64.3\n",
      "41 10.14 1.73 64.24\n",
      "42 10.14 1.71 64.18\n",
      "43 10.15 1.68 64.13\n",
      "44 10.16 1.66 64.07\n",
      "45 10.16 1.64 64.02\n",
      "46 10.17 1.62 63.97\n",
      "47 10.17 1.59 63.91\n",
      "48 10.18 1.57 63.86\n",
      "49 10.19 1.55 63.81\n",
      "50 10.19 1.53 63.75\n",
      "51 10.2 1.5 63.7\n",
      "52 10.2 1.48 63.65\n",
      "53 10.21 1.46 63.6\n",
      "54 10.22 1.44 63.55\n",
      "55 10.22 1.42 63.49\n",
      "56 10.23 1.4 63.44\n",
      "57 10.23 1.37 63.39\n",
      "58 10.24 1.35 63.34\n",
      "59 10.25 1.33 63.29\n",
      "60 10.25 1.31 63.25\n",
      "61 10.26 1.29 63.2\n",
      "62 10.26 1.27 63.15\n",
      "63 10.27 1.25 63.1\n",
      "64 10.28 1.22 63.05\n",
      "65 10.28 1.2 63.0\n",
      "66 10.29 1.18 62.96\n",
      "67 10.29 1.16 62.91\n",
      "68 10.3 1.14 62.86\n",
      "69 10.3 1.12 62.82\n",
      "70 10.31 1.1 62.77\n",
      "71 10.32 1.08 62.73\n",
      "72 10.32 1.06 62.68\n",
      "73 10.33 1.04 62.63\n",
      "74 10.33 1.02 62.59\n",
      "75 10.34 1.0 62.55\n",
      "76 10.34 0.98 62.5\n",
      "77 10.35 0.96 62.46\n",
      "78 10.36 0.94 62.41\n",
      "79 10.36 0.92 62.37\n",
      "80 10.37 0.9 62.33\n",
      "81 10.37 0.88 62.28\n",
      "82 10.38 0.86 62.24\n",
      "83 10.38 0.84 62.2\n",
      "84 10.39 0.82 62.16\n",
      "85 10.39 0.8 62.12\n",
      "86 10.4 0.78 62.08\n",
      "87 10.41 0.76 62.03\n",
      "88 10.41 0.74 61.99\n",
      "89 10.42 0.72 61.95\n",
      "90 10.42 0.7 61.91\n",
      "91 10.43 0.68 61.87\n",
      "92 10.43 0.66 61.83\n",
      "93 10.44 0.64 61.79\n",
      "94 10.44 0.62 61.75\n",
      "95 10.45 0.6 61.72\n",
      "96 10.45 0.58 61.68\n",
      "97 10.46 0.57 61.64\n",
      "98 10.46 0.55 61.6\n",
      "99 10.47 0.53 61.56\n"
     ]
    }
   ],
   "source": [
    "# Gives best result for l_rate=0.01 and iteration=1000.\n",
    "# You  want to see the cost going down.\n",
    "\n",
    "gradient_descent(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edc4e41-9e28-4e78-a303-36a897cd53a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dcd1411-903e-4a1b-ade1-4493564ffb50",
   "metadata": {},
   "source": [
    "# Choosing l_rate=0.01 and iteration=1000, gives b = 12 and m = -5 with lowest cost = 56 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a56119-8444-4664-ab24-bc10c3d481a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
